import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

# Load the MNIST dataset
mnist_data = keras.datasets.mnist.load_data()
(x_train, y_train), (x_test, y_test) = mnist_data

# Display some images
plt.imshow(x_train[0], cmap="gray")
plt.show()

# Normalize the pixel values to range [0, 1]
x_train = x_train / 255
x_test = x_test / 255

# Define the neural network
model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28, 28)))      # Flattens 28x28 image to 784 vector
model.add(keras.layers.Dense(128, activation='relu'))      # Hidden layer with 128 neurons
model.add(keras.layers.Dense(38, activation='relu'))       # Second hidden layer with 38 neurons
model.add(keras.layers.Dense(10, activation='softmax'))    # Output layer for 10 digit classes (0–9)

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model for 10 epochs with 20% validation data
model.fit(x_train, y_train, epochs=10, validation_split=0.2)

# Predict class probabilities for the test set
y_proba = model.predict(x_test)

# Convert probabilities to predicted class labels
y_preds = y_proba.argmax(axis=1)

# Calculate and print accuracy using sklearn
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_preds)

# ------------------------ WORKING EXPLANATION ------------------------

"""
WORKING:

1. Dataset:
   - Uses the MNIST dataset: 28x28 grayscale images of handwritten digits (0–9).
   - x_train and x_test contain image data.
   - y_train and y_test contain corresponding digit labels (0–9).

2. Preprocessing:
   - Pixel values are divided by 255 to scale them between 0 and 1 (normalization).

3. Model:
   - Sequential neural network with:
     a) Flatten layer: reshapes image from (28x28) to (784,)
     b) Dense layer with 128 neurons and ReLU activation
     c) Dense layer with 38 neurons and ReLU activation
     d) Output layer with 10 neurons (for digits 0 to 9) using softmax

4. Training:
   - Model is compiled using Adam optimizer and sparse categorical cross-entropy loss.
   - Trained for 10 epochs on training data (with 20% used for validation).

5. Prediction:
   - Model outputs class probabilities for test images.
   - `argmax` is used to convert probabilities to class labels.

6. Evaluation:
   - Final accuracy is computed using `accuracy_score()` comparing predictions to true labels.
"""

# ------------------------ VIVA QUESTIONS ------------------------

"""
VIVA QUESTIONS:

1. Q: What is the MNIST dataset?
   A: A dataset of 70,000 handwritten digit images (28x28 pixels), commonly used for image classification.

2. Q: Why do we normalize pixel values?
   A: To bring them into the range [0, 1], which helps the model converge faster during training.

3. Q: What does the Flatten layer do?
   A: It reshapes the 2D input image (28x28) into a 1D vector (784) to feed into dense layers.

4. Q: What activation function is used in the output layer, and why?
   A: Softmax is used to output probabilities for each of the 10 digit classes.

5. Q: Why use 'sparse_categorical_crossentropy'?
   A: Because the labels are integers (not one-hot encoded), and this loss handles that directly.

6. Q: What is the role of the Dense layers?
   A: They are fully connected layers that learn patterns and relationships in the data.

7. Q: What does 'validation_split=0.2' mean?
   A: It reserves 20% of training data for validation during training to monitor performance.

8. Q: What is the purpose of 'argmax' after prediction?
   A: It selects the class with the highest predicted probability for each image.
"""

