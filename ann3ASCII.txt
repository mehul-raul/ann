import numpy as np

# Define the dataset
# Each digit from '0' to '9' represented in 7-bit ASCII binary
inputs = [
    [0,1,1,0,0,0,0],  # '0' = 48 â†’ Even
    [0,1,1,0,0,0,1],  # '1' = 49 â†’ Odd
    [0,1,1,0,0,1,0],  # '2' = 50 â†’ Even
    [0,1,1,0,0,1,1],  # '3' = 51 â†’ Odd
    [0,1,1,0,1,0,0],  # '4' = 52 â†’ Even
    [0,1,1,0,1,0,1],  # '5' = 53 â†’ Odd
    [0,1,1,0,1,1,0],  # '6' = 54 â†’ Even
    [0,1,1,0,1,1,1],  # '7' = 55 â†’ Odd
    [0,1,1,1,0,0,0],  # '8' = 56 â†’ Even
    [0,1,1,1,0,0,1],  # '9' = 57 â†’ Odd
]

# Labels: 0 for Even, 1 for Odd
labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]

# Convert to NumPy arrays
X = np.array(inputs)
y = np.array(labels)

# Initialize weights and bias
weights = np.zeros(X.shape[1])
bias = 0
learning_rate = 0.1
epochs = 10

# ------------------ TRAINING PHASE ------------------ #
for epoch in range(epochs):
    for i in range(len(X)):
        z = np.dot(X[i], weights) + bias
        prediction = 1 if z >= 0 else 0  # Step function
        error = y[i] - prediction
        weights += learning_rate * error * X[i]
        bias += learning_rate * error

# ------------------ MANUAL INPUT PHASE ------------------ #
digit = int(input("Enter a digit (0-9): "))
if 0 <= digit <= 9:
    ascii_code = ord(str(digit))         # Get ASCII value of character
    binary_input = [int(b) for b in format(ascii_code, '07b')]  # 7-bit binary
    z = np.dot(binary_input, weights) + bias
    prediction = 1 if z >= 0 else 0
    print(f"Prediction: {'Odd' if prediction == 1 else 'Even'}")
else:
    print("Invalid input. Please enter a digit between 0 and 9.")

# ------------------ COMMENTS & EXPLANATION ------------------ #

"""
HOW THIS CODE WORKS:

1. DATA:
   - It uses the ASCII binary representations of digits '0' to '9'.
   - Each character (e.g., '0') is converted to its 7-bit ASCII binary form.
   - These binary inputs are associated with labels: 0 (even) or 1 (odd).

2. MODEL:
   - A simple perceptron (a basic neural unit) is trained.
   - It has 7 weights (for 7 input bits) and 1 bias.
   - The perceptron uses a step function: if weighted sum â‰¥ 0, it predicts 1 (odd), else 0 (even).

3. TRAINING:
   - The perceptron updates its weights and bias when it makes a wrong prediction.
   - This is done using the Perceptron Learning Rule:
        weights += learning_rate * error * input
        bias   += learning_rate * error

4. TESTING:
   - After training, the user can input a digit (0â€“9).
   - The program converts it to ASCII binary, feeds it to the model, and predicts "Even" or "Odd".

WHY THIS WORKS:
   - The least significant bit (LSB) of an ASCII-encoded digit is a strong indicator of whether the digit is even or odd.
   - The perceptron learns this pattern during training.

LIMITATIONS:
   - This model only handles digits 0â€“9, not general numbers or characters.
"""


# ðŸ§  Activation Function:
# Step function: Output is 1 if net input â‰¥ 0; else 0.

# ðŸŽ¯ Goal:
# Output 0 for even digits and 1 for odd digits.

# ðŸ“˜ VIVA Questions and Answers (as comments):

# Q1: What is a perceptron?
# A: A perceptron is a type of artificial neuron that makes decisions by weighing input signals, summing them, and applying a threshold-based activation function.

# Q2: Why is ASCII used here?
# A: ASCII representation helps convert characters (digits) into binary input features that the perceptron can process.

# Q3: Why does this problem work with a single-layer perceptron?
# A: Because even/odd classification in ASCII is linearly separable â€” mainly based on the last bit of ASCII.

# Q4: What is the learning rule used here?
# A: Perceptron learning rule: weights are updated based on the error (actual - predicted) multiplied by learning rate and input.

# Q5: Can a perceptron solve XOR?
# A: No, XOR is not linearly separable, so it cannot be solved by a single-layer perceptron.

# Q6: What happens if we increase the number of epochs?
# A: More epochs may improve learning and convergence, but since this is a simple problem, it usually learns quickly.

# Q7: What is the role of bias in a perceptron?
# A: Bias shifts the activation function left or right, helping the perceptron learn patterns that don't pass through the origin.

# Let me know if you'd like a graphical visualization or an extension of this into a multilayer perceptron using libraries like Keras or PyTorch.
