import numpy as np

# Define the dataset
# Digits 0 to 9 in 7-bit ASCII binary representation
# Example: ASCII('0') = 48 = 0b0110000
inputs = [
    [0,1,1,0,0,0,0],  # '0' (ASCII 48) â†’ Even
    [0,1,1,0,0,0,1],  # '1' (ASCII 49) â†’ Odd
    [0,1,1,0,0,1,0],  # '2' (ASCII 50) â†’ Even
    [0,1,1,0,0,1,1],  # '3' (ASCII 51) â†’ Odd
    [0,1,1,0,1,0,0],  # '4' (ASCII 52) â†’ Even
    [0,1,1,0,1,0,1],  # '5' (ASCII 53) â†’ Odd
    [0,1,1,0,1,1,0],  # '6' (ASCII 54) â†’ Even
    [0,1,1,0,1,1,1],  # '7' (ASCII 55) â†’ Odd
    [0,1,1,1,0,0,0],  # '8' (ASCII 56) â†’ Even
    [0,1,1,1,0,0,1],  # '9' (ASCII 57) â†’ Odd
]

# Labels: 0 for even, 1 for odd
labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]

# Convert to NumPy arrays for easier computation
X = np.array(inputs)
y = np.array(labels)

# Initialize weights and bias
weights = np.zeros(X.shape[1])
bias = 0
learning_rate = 0.1
epochs = 10  # Number of training passes

# ------------------ TRAINING PHASE ------------------ #
for epoch in range(epochs):
    for i in range(len(X)):
        # Compute weighted sum
        z = np.dot(X[i], weights) + bias
        # Activation function: step (threshold at 0)
        prediction = 1 if z >= 0 else 0
        # Update weights and bias if prediction is wrong
        error = y[i] - prediction
        weights += learning_rate * error * X[i]
        bias += learning_rate * error

# ------------------ TESTING PHASE ------------------ #
print("Digit | Prediction (0=Even, 1=Odd)")
for i in range(len(X)):
    z = np.dot(X[i], weights) + bias
    prediction = 1 if z >= 0 else 0
    print(f"  {i}   |      {prediction}")

# ------------------ COMMENTS & EXPLANATION ------------------ #

# ðŸ’¡ Working:
# This perceptron learns to distinguish even and odd digits (0â€“9) by analyzing their ASCII binary forms.
# The ASCII values for '0'â€“'9' range from 48 to 57, and only the last bit (LSB) of ASCII changes between even and odd.
# Thus, a simple perceptron can learn this separation as it's linearly separable.

# ðŸ§  Activation Function:
# Step function: Output is 1 if net input â‰¥ 0; else 0.

# ðŸŽ¯ Goal:
# Output 0 for even digits and 1 for odd digits.

# ðŸ“˜ VIVA Questions and Answers (as comments):

# Q1: What is a perceptron?
# A: A perceptron is a type of artificial neuron that makes decisions by weighing input signals, summing them, and applying a threshold-based activation function.

# Q2: Why is ASCII used here?
# A: ASCII representation helps convert characters (digits) into binary input features that the perceptron can process.

# Q3: Why does this problem work with a single-layer perceptron?
# A: Because even/odd classification in ASCII is linearly separable â€” mainly based on the last bit of ASCII.

# Q4: What is the learning rule used here?
# A: Perceptron learning rule: weights are updated based on the error (actual - predicted) multiplied by learning rate and input.

# Q5: Can a perceptron solve XOR?
# A: No, XOR is not linearly separable, so it cannot be solved by a single-layer perceptron.

# Q6: What happens if we increase the number of epochs?
# A: More epochs may improve learning and convergence, but since this is a simple problem, it usually learns quickly.

# Q7: What is the role of bias in a perceptron?
# A: Bias shifts the activation function left or right, helping the perceptron learn patterns that don't pass through the origin.

# Let me know if you'd like a graphical visualization or an extension of this into a multilayer perceptron using libraries like Keras or PyTorch.
